import os
# if using Apple MPS, fall back to CPU for unsupported ops
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image
from sam2.build_sam import build_sam2
from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

# # select the device for computation
# if torch.cuda.is_available():
#     device = torch.device("cuda")
# elif torch.backends.mps.is_available():
#     device = torch.device("mps")
#else:
device = torch.device("cpu")
print(f"using device: {device}")

if device.type == "cuda":
    # use bfloat16 for the entire notebook
    torch.autocast("cuda", dtype=torch.bfloat16).__enter__()
    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)
    if torch.cuda.get_device_properties(0).major >= 8:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
elif device.type == "mps":
    print(
        "\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might "
        "give numerically different outputs and sometimes degraded performance on MPS. "
        "See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion."
    )

np.random.seed(3)

def show_anns(anns, borders=True):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)

    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:, :, 3] = 0
    for ann in sorted_anns:
        m = ann['segmentation']
        color_mask = np.concatenate([np.random.random(3), [0.5]])
        img[m] = color_mask 
        if borders:
            import cv2
            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) 
            # Try to smooth contours
            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]
            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1) 

    ax.imshow(img)

def generate_masks(image):

    sam2_checkpoint = "checkpoints/sam2.1_hiera_large.pt"
    model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"

    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)
    mask_generator = SAM2AutomaticMaskGenerator(sam2)
    #    model=sam2,
    #    points_per_side=32,
    #    points_per_batch=64,
    #    pred_iou_thresh=0.8,
    #    stability_score_thresh=0.95,
    #    stability_score_offset=1.0,
    #    crop_n_layers=1,
    #    box_nms_thresh=0.7,
    #    crop_n_points_downscale_factor=2,
    #    min_mask_region_area=25.0,
    #    use_m2m=True)

    masks = mask_generator.generate(image)

    return masks

def generate_masked_images(image, masks, output_dir="masked_images"):
    """
    Extract only the masked portions of the original image and make the rest transparent.
    
    Args:
        image: The original image as a numpy array
        masks: List of masks generated by SAM2
        output_dir: Directory to save masked images
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Convert numpy array to PIL Image for easier manipulation
    original_img = Image.fromarray(image)
    
    # Process each mask
    for i, mask in enumerate(masks):
        # Get the mask segmentation
        segmentation = mask['segmentation']
        
        # Create an RGBA version of the original image
        masked_img = original_img.copy().convert("RGBA")
        
        # Create a transparent mask (all pixels are transparent)
        transparent_mask = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)
        
        # Only copy the RGB values from the original image for pixels where the mask is True
        # and set alpha to 255 (fully opaque) for those pixels
        transparent_mask[segmentation, :3] = np.array(image)[segmentation]
        transparent_mask[segmentation, 3] = 255
        
        # Convert the masked array to PIL Image
        masked_obj_img = Image.fromarray(transparent_mask)
        
        # Save as webp with high quality
        output_path = os.path.join(output_dir, f"mask_{i}.webp")
        masked_obj_img.save(output_path, format="WEBP", quality=95)
        
        print(f"Saved masked image to: {output_path}")
    
    # Also create a version with all masks, each with a different color
    all_masks_img = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)
    
    # Sort masks by area (as in show_anns)
    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)
    
    # Extract each mask with a different colored background
    for i, mask in enumerate(sorted_masks):
        segmentation = mask['segmentation']
        
        # Generate a random color for this mask's background
        color = np.concatenate([np.random.randint(0, 256, 3), [255]])  # RGBA with full opacity
        
        # Only update pixels that haven't been assigned yet (original value is 0)
        # This ensures larger objects (earlier in sorted_masks) take precedence
        unassigned = (all_masks_img[:, :, 3] == 0)
        combined_mask = np.logical_and(segmentation, unassigned)
        
        if np.any(combined_mask):  # Only update if there are pixels to modify
            # Copy RGB data from the original image for this mask
            all_masks_img[combined_mask, :3] = np.array(image)[combined_mask]
            all_masks_img[combined_mask, 3] = 255  # Fully opaque
    
    all_masks_path = os.path.join(output_dir, "all_masks.webp")
    Image.fromarray(all_masks_img).save(all_masks_path, format="WEBP", quality=95)
    
    print(f"Saved combined mask image to: {all_masks_path}")
    
    return output_dir

def generate_blurry_images(image, image_dirs, output_dir="blurry_combinations"):
    """
    Generate all possible combinations of blurry and non-blurry segments into images.
    Starts with the fully blurred image and selectively unblurs sections not covered by masks.
    
    Args:
        image: Original image as numpy array
        image_dirs: Dictionary mapping keywords to their masked image paths.
                    Each value can be a single path string or a list of paths.
                    Multiple masks for a single keyword will be treated as a group.
        output_dir: Directory to save the generated combinations
    """
    import itertools
    from PIL import Image, ImageFilter, ImageChops
    
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Create base images: original and fully blurred
    original_pil = Image.fromarray(image).convert("RGBA")
    blurred_pil = original_pil.copy().filter(ImageFilter.GaussianBlur(radius=10))
    
    # Process keywords and their associated masks
    keywords = list(image_dirs.keys())
    keyword_masks = []
    
    # Load all masks and group them by keyword
    for keyword in keywords:
        paths = image_dirs[keyword]
        if isinstance(paths, str):
            paths = [paths]
            
        # Load all masks for this keyword
        masks_for_keyword = []
        for path in paths:
            masks_for_keyword.append(Image.open(path))
        
        keyword_masks.append(masks_for_keyword)
    
    # Generate all possible keyword combinations
    num_keywords = len(keywords)
    
    # Total combinations = 2^n - 1 (excluding the case where no keywords are applied)
    total_combinations = 2**num_keywords - 1
    print(f"Generating {total_combinations} combinations of blurry and non-blurry images")
    
    # Generate all combinations of keywords to blur
    for r in range(1, num_keywords + 1):  # Start from 1 to exclude empty combo
        for keyword_indices in itertools.combinations(range(num_keywords), r):
            # Build filename parts based on which keywords are blurred
            name_parts = []
            
            # Create a composite mask for all sections to be blurred
            composite_mask = Image.new("L", original_pil.size, 0)
            
            # Process each selected keyword
            for idx in keyword_indices:
                # Add keyword index to filename
                name_parts.append(f"{idx}blur")
                
                # Process all masks for this keyword
                for mask in keyword_masks[idx]:
                    # Extract alpha channel from the mask
                    if mask.mode == 'RGBA':
                        mask_alpha = mask.split()[3]
                        
                        # Add this mask to our composite mask using maximum value
                        composite_mask = ImageChops.lighter(composite_mask, mask_alpha)
            
            # Add non-blurred keyword indices to the filename
            for i in range(num_keywords):
                if i not in keyword_indices:
                    name_parts.append(str(i))
            name_parts.sort()  # Sort for consistency
            
            # Create an inverse mask (areas that should NOT be blurred)
            inverse_mask = ImageChops.invert(composite_mask)
            
            # Start with blurred image and overlay original image using the inverse mask
            # This will keep the blurred parts where the mask was active and use original image elsewhere
            result_img = Image.composite(original_pil, blurred_pil, inverse_mask)
            
            # Save the result
            output_path = os.path.join(output_dir, f"{'_'.join(name_parts)}.webp")
            result_img.save(output_path, format="WEBP", quality=95)
            print(f"Saved combination: {output_path}")
    
    print(f"All {total_combinations} combinations generated in {output_dir}")
    return output_dir

if __name__ == "__main__":    
    image = Image.open('../frontend/public/random-1/original_image.webp')
    image = np.array(image.convert("RGB"))
    #masks = generate_masks(image)
    ## Generate and save masked images
    #output_dir = generate_masked_images(image, masks)
    
    # Example with multiple masks per keyword
    image_dirs = {
        "sad": ["masked_images/mask_29.webp"], 
        "ice": ["masked_images/mask_3.webp"],
        "sculpture": ["masked_images/mask_16.webp"],
        "computer": ["masked_images/mask_8.webp", "masked_images/mask_28.webp"],  # Multiple masks
        "underwater": ["masked_images/mask_18.webp"]  # Multiple masks
    }
    generate_blurry_images(image, image_dirs)